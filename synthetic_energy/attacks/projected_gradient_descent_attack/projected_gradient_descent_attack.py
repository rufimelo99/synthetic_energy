from typing import List, Optional

import torch
import torch.nn.functional as F
from tqdm import tqdm

from synthetic_energy.attacks.fast_gradient_sign_attack.fast_gradient_sign_attack import (
    DenormalizingTransformation,
    FastGradientSignAttack,
)


class ProjectedGradientDescent(FastGradientSignAttack):
    """
    Projected Gradient Descent (PGD) Attack.

    The Projected Gradient Descent (PGD) Attack is an iterative adversarial attack
    method that enhances the Fast Gradient Sign Attack (FGSM) by applying it multiple times
    with a small step size. Introduced by Madry et al. in "Towards Deep Learning Models
    Resistant to Adversarial Attacks," PGD generates stronger adversarial examples by
    refining the perturbations iteratively, making it a widely used technique for
    evaluating the robustness of neural networks against adversarial attacks.
    """

    def __init__(
        self,
        epsilon: float,
        device: torch.device,
        num_steps: int,
    ) -> None:
        """
        Initializes the Projected Gradient Descent Attack.

        Parameters
        ----------
        epsilon : float
            The maximum allowable perturbation applied to the input data.
        device : torch.device
            The device (CPU or GPU) on which the attack computations will be executed.
        num_steps : int
            The number of iterations to apply the attack, defining how many times
            the gradient descent update will be projected onto the feasible region.
        """
        super().__init__(epsilon=epsilon, device=device)
        self.num_steps = num_steps

    def attack(
        self,
        model: torch.nn.Module,
        dataloader: torch.utils.data.DataLoader,
        denormlizing_transform: Optional[DenormalizingTransformation] = None,
    ) -> List:
        """
        Performs the Projected Gradient Descent Attack on the specified model, generating
        adversarial examples from the input data provided by the DataLoader.

        This method iteratively applies the PGD attack on each batch of input data,
        adjusting the inputs to maximize the loss while staying within the specified
        perturbation bounds defined by epsilon.

        Parameters
        ----------
        model : torch.nn.Module
            The neural network model to attack, which will process the input data to produce outputs.
        dataloader : torch.utils.data.DataLoader
            The DataLoader that supplies batches of input data for the attack.
        denormlizing_transform : Optional[DenormalizingTransformation], optional
            A transformation to reverse normalization applied to the input data,
            if needed for visualizing or further processing the adversarial examples.

        Returns
        -------
        List
            A list containing the adversarial examples generated by the PGD attack
            for each input in the DataLoader.
        """
        model.eval()
        adversarial_examples = []

        for inputs, labels in tqdm(dataloader, desc="Attacking"):
            inputs, labels = inputs.to(self.device), labels.to(self.device)

            if denormlizing_transform:
                inputs = self._denorm(
                    inputs, denormlizing_transform.mean, denormlizing_transform.std
                )

            # Generate adversarial examples using PGD
            perturbed_inputs = self._pgd_attack(model, inputs, labels)

            adversarial_examples.append(perturbed_inputs)

        return adversarial_examples, adversarial_examples

    def _pgd_attack(
        self,
        model: torch.nn.Module,
        inputs: torch.Tensor,
        labels: torch.Tensor,
    ) -> torch.Tensor:
        """
        Performs the Projected Gradient Descent (PGD) Attack on the provided input data.

        This method applies the PGD attack iteratively, adjusting the input data based on the
        gradients of the loss function with respect to the inputs, while ensuring that the
        perturbations remain within the specified epsilon bounds. This allows for the
        generation of adversarial examples that are designed to mislead the model.

        Parameters
        ----------
        model : torch.nn.Module
            The neural network model to attack, which processes the input data to produce predictions.
        inputs : torch.Tensor
            The input data that will be perturbed to create adversarial examples.
        labels : torch.Tensor
            The true labels corresponding to the input data, used to compute the loss.

        Returns
        -------
        torch.Tensor
            The perturbed input data, which represents the adversarial examples generated
            by the PGD attack.
        """
        # Initialize the perturbed inputs as the original inputs
        perturbed_inputs = inputs.clone().detach()

        # Set requires_grad attribute of tensor
        perturbed_inputs.requires_grad = True

        # Perform PGD Attack
        for _ in range(self.num_steps):
            # Forward pass the perturbed inputs through the model
            outputs = model(perturbed_inputs)

            # Calculate the loss
            loss = F.nll_loss(outputs, labels)

            # Zero the gradients
            model.zero_grad()

            # Backward pass to calculate the gradients
            loss.backward()

            # Collect the data gradients
            data_grad = perturbed_inputs.grad.data

            # Perform the FGSM update
            perturbed_inputs = self._fgsm_attack(
                perturbed_inputs, self.epsilon, data_grad
            )

        return perturbed_inputs
